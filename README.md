# UFRJ B.Sc Thesis - PHOTOMETRIC IDENTIFICATION OF SUPERNOVAS THROUGH MACHINE LEARNING ALGORITHMS


Undergraduate Project presented to POLI/UFRJ as a partial fulfillment of the requirements for the degree of Engineer.

SUPERNOVA PHOTOMETRIC IDENTIFICATION USING MACHINE LEARNING ALGORYTHMS

Felipe Matheus Fernandes de Oliveira

August/2019

Advisors: Amit Bhaya
Ribamar Rondon de Rezende dos Reis

Course: Automation and Control Engineering

**Abstract** 

**In order to study the expansion of the universe, cosmology classifies different types of astronomical objects using spectroscopy. Given the enourmous size of current datasets, spectroscopy methods could not classify this amount of data. As a
solution to this issue, photometric identification is crucial to fully exploit these large samples due to its simplicity. Once photometric identification uses machine learning algorithms, the following work tries to optimize those algorithms.**

# Chapter 1. Introduction
## 1.1. Theme and Contextualization
  In cosmology there is a need to determine distances (through what is called luminosity distance [3]) to shape its studies. To reach this purpose, light curves originated from Ia supernovae are used as tools.
  
  Given that light curves are flow measurements (energy per time per area [4]) against time, in order to determine the distance of an object from this measure we need to know the intrinsic potency of it. An object which has this potency is called a standard candle [5]. Even though supernovas are not standard candles, the ones that are Ia type can be standardized through empirical correlations among its observable features. This standardization process can be observed exclusively in this type of Supernova, which makes these objects’ precise categorization crucial.
  
  In former times the data set of supernovas used to be little enough to enable the analysis of the most part of the objects using spectroscopy [6], a method that provides flow measurements according to its wave-lenght or frequency. Despite spectroscopy precisely confirming the type of each supernova, the large amount of data we have nowadays makes it a slow and expensive method.
  
  As researches and telescopes are technologically advancing, Astronomy has been moving forward to an era of massive data sets, which makes necessary the use of simpler and more practical automated techniques for through spectroscopy that would no longer be possible.
  
  In this scenario different approaches were developed to catalog this big amount of obtained objects. Among them, there are plenty making use of machine learning.
  
  Finally, the idea of this project came from an article published by MICHELLE LOCHNER [7] and associates. The author seeks creating an automatic form of photometric classification making use of the light curves obtained through photometry [8], a method that gives flow measurements in broadband filters (typically 1000 Angstroms), where those curves have already been properly arranged by spectroscopy in the past. 
  
  To sum up, it is about using a faster method to obtain less information about astronomical objects and, by having a precise arrangement of them, training a machine learning method so it can accurately classify them, even if we have less data.

## 1.2. Issues
  In the present study, the term “pipeline” will be used several times to refer to a sequence of steps or procedures/algorithms used over the process of classification, portraying the construction of the process stages as a whole. 
  
  Since the article [7] tested and validated several pipelines, this study adopted the one which obtained the best ranking result and, from that point, changes were made to evaluate any possible improvement. 
  
  The chosen pipeline is mostly built by 4 parts:
- Gaussian Process (GP) Interpolation.
- Wavelet Transform of the interpolated functions.
- Principal components analysis (PCA) of the parameters generated by the Wavelet Transform.
- Random Forest (RF) applied to the classification of the astronomical objects through its principal components (PCA result).

The main issue lies in the interpolation method called Gaussian Process. Since it is an interpolation method, we hope it establishes a function that assumes values from original data, respecting the uncertainties in its measurements. However, where we hoped to see a temporal variation of considerable flowing, in some cases the interpolated chart is a constant (Figure 1.1). As a result, that interpolation does not have a physical sense as it refers to the flow of light after an astronomical object’s explosion in addition to violating the second law of thermodynamics [9], for an explosion which has a constant flow of light would represent an object of infinite energy. 

The main purpose of this work is to propose a modification of the interpolation method making use of GP that avoids the behaviors which do not match the scientific laws. 

Alongside with that, other ideas have been applied in an attempt to obtain improvements in the algorithm currently used by the _Instituto de Física da UFRJ (IF-UFRJ)_.


![image](https://github.com/FelipeMFO/supernovae-classification/assets/38300024/c16e43c9-4d2c-4a3d-b777-a1150b04d5c1)

Figure 1.1: Example of an interpolation via GP which does not fit the reality of an explosion.


## 1.3. Delimitation
  All data used in this work as well as in the article [7] have been extracted from the database offered by the KESSLER [10] challenge. This challenge, proposed by the Argonne National Laboratory [11] opened up the issue of the categorization of supernovas to the public in order to obtain better results and/or new algorithms. The database is of public domain and it can be searched in the challenge's repository. 
  
  Due to the granularity and abundance of raw data, it was necessary a pre-treatment looking only for the data that will be used. This pretreatment was taken from the pipeline of the Instituto de Física da UFRJ, which aims to read the text files transforming each information into a dictionary key.
  
  By the end of this data selection we obtained values in the form of Python dictionaries for each astronomical object. These dictionaries will be the raw data inside the scope of this work. 
## 1.4. Objectives
  The main purpose of this work is to optimize certain aspects of the currently used pipeline after an initial study was made to identify weak points or possibilities of improvement. These aspects will be reported and justified below, creating the three aspects of the project.

- Treatment of Outliers.
- Deep Learning.
- Interpolation using Gaussian Process.


### 1.4.1. Treatment of Outliers 
  The last strand to seek improvements in the algorithm is the treatment of the aspects whose uncertainty is too high or whose values are other than what is expected . Aspects with these traits are called outliers in the literature. 
  
  By analyzing some raw data, it is possible to observe aspects whose uncertainty extrapolate the highest value of the curve (the highest point to the left in the figure 1.3) and also points that are found completely out of the interpolated curve (figure 1.2).
  
  The steps of the treatment are presented in Chapter 4.

![image](https://github.com/FelipeMFO/supernovae-classification/assets/38300024/fe9c0d7e-2a78-461e-98ae-b21144dc9a16)

Figure 1.2: Example of outlier. Graphic Flow x Time (days).

![image](https://github.com/FelipeMFO/supernovae-classification/assets/38300024/3cd61aa1-de47-4b0f-afc8-a416c811809b)

Figure 1.3: Example of point with uncertainty 3 times bigger than the peak of the curve. Graphic Flow x Time (days).

### 1.4.2. Deep Learning

Deep learning (DL) has been used recently in order to improve certain Machine Learning (ML) algorithms. 

The idea of using DL in this matter came from the analysis of the initial pipeline. Regarding that it has various parts between the raw data and the Random Forest algorithm, we apply DL expecting to obtain a simpler pipeline, containing only one step between the raw data and the classifying algorithm.

In many other cases in the literature, such as identification of sounds or images, there were significant improvements when reduced the number of steps regarding signal processing (wavelets) or reduction of components (PCA) in exchange of using DL.

The second reason for applying DL came from MICHELLE LOCHNER’s[7] article, in which she mentions possible applications of it in the issue’s scope. 

Thus, it was decided to apply DL algorithms after the first part of data treatment, aiming to keep a simplicity through the pipeline, establishing only the GP stage between the raw data and the classifying algorithm. The detailing of this procedure is in Chapter 5.

### 1.4.3. Gaussian Process
  The Gaussian Process is the first part of the treatment after the conversion of raw data into Python dictionaries. This step will be approached in Chapter 6. 
  
  Each astronomic object has an amount of points that represents the intensity of the flow of light captured in a certain day and in a certain filter. Linked to each point there is also an uncertainty of this value. Thus, we look for an interpolation that is a considerable temporal variation, passing by certain points. 
  
  The purpose in addressing this issue is to to fix interpolations which do not match the physical interpretation, as it can be seen in the figure 1.1. 
   
## 1.5. Methodology
On account of the pre-processing code from IF-UFRJ being written in Python, the entire work was also written in the same language. Another reason for its choice was its ease to make experiments through the Jupyter hub, aside from its great amount of data science library and support for developing ML algorithms.

The main libraries used in the data treatment’s scenario were Pandas [12] and Numpy [13]. When it comes to the application of the methods, the most important libraries were Scikit-learn [14], Keras [15] and PyMC3 [16].

The model’s inputs will always be the dictionaries generated from the raw data after the pre-processing of the IF-UFRJ. Therefore, we could easily refine the desired properties for each one of all the three aspects of the work. 

Each one of the aspects has a specific and isolated work so that we can better check its changes afterwards. In other words, to each change there was a direct comparison with the results of the original pipeline, so we were able to specifically evaluate the efficiency and improvement of that change.

Since it is a classifying algorithm, the model’s output will be boolean and will classify if an astronomical object is Ia type or not. However, in raw data, classes are divided between 8 types of objects: Ia, Ib, Ibc, Ic, II, IIL, IIN and IIP. 

It is also important to mention that, in order to validate the models from the project, the K-Fold method was used. Nevertheless, this work distinguished itself from the classic division which lies in the literature. Usually, 80% of data goes for training and 20% goes for the model tests. In this project only 1100 objects were used for training, among the 21316 total objects. The excuse for that is the physical limitation of the samples (only 1100 spectroscopically confirmed objects will be known).

At last, changes were verified through Confusion Matrices (CM) or through score methods from the Scikit-learn library.

As a final observation, aiming to perform subsequent studies outside the project’s scope, classifications concerning the 8 types of objects were made, as well as practices adopting the proportion of 80% for training and 20% for testing.
  
## 1.6. Description
 In Chapter 2 the main theoretical concepts used in the application of the Gaussian Process and in Deep Learning will be explained. There also will be raised some concepts that are not the focus of the work, but they are equally significant to it. 

Chapter 3 will be destined to detail specifically each step of the current pipeline, as a brief mention to the way in which the raw data are treated. 

Chapters 4, 5 and 6 will be destined to the explanation of the goals mentioned in section 1.4, as well as the results of their changes, seeking to ascertain the efficiency of the application or to explain possible errors.

Finally, in Chapter 7, the final conclusions of the project and some suggestions of future works will be presented alongside with the restrictions and the possible solutions found. 

# Chapter 2. Theoretical foundations
## 2.1. Gaussian Process
The Gaussian Process is an important tool for Machine Learning algorithms for that allows it to make predictions over the data based on an a priori knowledge . Its most usual application is in function interpolations, same as this project’s case. There are also possible applications of this concept in classifications and groupings of great amounts of data. The reference book used for this project studies was RASMUSSEN and WILLIAMS [17].

As for a certain amount of points there are infinite functions that can interpolate these values, the Gaussian Process performs its interpolation from the a priori values expectation and the format that it may take due to the connection between the points. Lastly, the GP will not obtain a specific value for each point of the interpolated function but a probabilistic distribution for each point, in which each one will have an average (the interpolation value) and a standard deviation (uncertainty).

In this project it will not be approached further details for the learning of the method. However, in the bibliography it is possible to find more specific references and techniques [17], just as much as more practical ones [18], [19], [20].

Both of the two fundamental aspects of the Gaussian Process are the main sculptors for characterizing the interpolations. They will be presented in the following subsections.
    
### 2.1.1. Kernel and Function of Covariance
The Gaussian Process interpoles a discrete function of n points through the m known points, which results in n normal distributions, one for each point that will build this discrete function.

The differential of the GP is to consider a n + m dimensional multivariable normal distribution during the interpolation. This multivariable normal distribution will have a Σ covariance that is responsible not only for describing the format (sine wave, linear), but also for determining characteristics (frequency, rate of change) of the function to be predicted.

To be able to obtain the covariance matrix (Σ) of this multivariable normal, the GP’s Kernel is established. This is a special function of two variables that respects certain mathematical restrictions (Chapter 4 RASMUSSEN and WILLIAMS [17]). These two variables are the values of the abscissa of each point of the function to be interpolated. 

In other words, Kernel is a function that will relate two points that must be in the interpolated curve, based solely on the distance between them (for stationary Kernels) or in its absolute values (for non-stationary Kernels). This is the relation that will provide characteristics as frequency, spikes and curve smoothing. 

It is worth highlighting that only the abscissa values will be used for this relation, meanwhile the ordinary values of the points which are already known will be used to make this function to pass across them, through conditional probability (Posterior Distribution [20]). This method of conditional probability can be used due to its Gaussian distributions property, which settles: conditional of marginalized distributions that came from Gaussian distributions are Gaussian as well.

The figure 2.1 shows an easier way of viewing what was said in the previous paragraph. We can see in figure 2.1(b) that the probability of the values and of the 10 points we wish to interpole will be calculated through conditional probability considering the fixed values of the 2 pre-set points . That differs from what is illustrated in figure 2.1(a), in which we did not use conditional probability for the calculation of the values.

![image](https://github.com/FelipeMFO/supernovae-classification/assets/38300024/d09a44e7-f5a6-4941-bb31-91c383c49c8e)

(a) Illustration of the Σ matrix to 10 points.

![image](https://github.com/FelipeMFO/supernovae-classification/assets/38300024/c298484a-d10e-425c-99b2-07fc1265bb1f)

(b) Illustration of the Σ matrix to 12 points. 2 of them are given points.

Figure 2.1: Illustration of the Σ matrix, which has 10 and 12 points. Images from JOCHEN GÖRTLER [20].

   
### 2.1.2. Average function 
 The average function of a Gaussian Process is the one responsible for offering the initial prediction of the point which will be interpolated. It is the value that a certain point would have only if this one alone was to be defined.
  
  However, it only makes sense to talk about the average function in case it is associated with the Function of Covariance. Ordinarily, the Average Function is established as a normal distribution with an average 0 and 1 as standard deviation. 
  
  We can use this standard distribution as an value a priori, after all this point’s final value will be obtained after having a number of samples large enough to optimize the distribution. They are defined by samples in which we use the Markov Chain Monte Carlo [21], increasingly getting closer to the ideal value. At last, the point’s value will be the average of this distribution a posteriori. The figure 2.2(a) illustrates this scenario.
  
  Nonetheless, the appropriate use of the Average Function is crucial, for if it has its domains limited it may prevent the function from taking some values, as it is illustrated in figure 2.2(b).

![image](https://github.com/FelipeMFO/supernovae-classification/assets/38300024/8e63d4e1-6ff7-4109-8239-f037c4b477da)

(a) Example of a Gaussian priori which starts to assume the real value over the samples (in the example's case that is 4).

![image](https://github.com/FelipeMFO/supernovae-classification/assets/38300024/5eca2c51-4510-40f0-b30f-b54cfa4594dc)

(b) Example of a delimited priori that, even over the samples, will never assume its real value (in the example's case that is 4 and the delimitation is between -2 and 2).
Figure 2.2: Illustration of the value of the Average Function during the samples via Markov Monte Carlo Chain. Images from BAILEY [19].

## 2.2. Convolutional neural network
  Convolutional neural networks (CNN) are a type of neural network that is applied in Deep Learning and it is mostly used for image analysis. The main purpose of this project is to use Machine Learning algorithms. Thus, in order to avoid any misconceptions regarding the nomenclatures, it is important to keep in mind that DL refers to a technique inside ML’s large field. 
  
  A basic DL’s architecture has 3 parts, as it can be seen in the figure 2.3:
  
- Input Layer: the first layer; the one responsible for receiving a data's sample and propagating it to the internal layers.
- Hidden Layers: the group of internal layers; we can find in it the knots which receive the data from the input layer and connect to the following layers through non-linear Activation functions.
- Output layer: the last layer; the one responsible for the final answer. In a classifying network it has numerical output values that refers to the classes. In this project's case those are the numbers zero and one, matching the supernovas classified as Ia or not Ia.

Each layer is formed by different types of neurons.These are the mathematical functions with specific parameters which will be trained so we can classify the data.

![image](https://github.com/FelipeMFO/supernovae-classification/assets/38300024/935aec52-cb3a-43a9-8cbc-d6e4d429e724)

Figure 2.3: Basic DL’s Architecture. Images from IVAN [22].

CNN is a class of neural networks frequently applied to image’s analysis. In this project, the images are .png files created through Gaussian interpolation, which will be presented in matrix of values, as it can be seen in the current blue matrix in figure 2.4.

Convolutional neural networks are usually referred to as totally connected networks. In other words, each neuron in a layer is connected to every neuron in the next layer. This high connectivity makes the algorithm demand the minimal preprocessing possible when compared to other image’s classifying methods so that, in case enough samples are given, the network “learns” the filters that in a traditional algorithm would have to be manually implemented [23]. In CNN there are also used multilayer perceptrons [24]. The function of each neuron can be understood as the green matrix’s illustration in figure 2.4 and each parameter being the number that multiplies the value of the blue matrix.

![image](https://github.com/FelipeMFO/supernovae-classification/assets/38300024/8436738d-538e-4c1b-a827-674d6ad41244)

Figure 2.4: The blue matrix represents input variables; the red one represents results of the convolutional operation; the green one has the parameters to multiply values of the blue matrix, “sliding” through them. Images from DERTAT [25].

CNNs can have layers of pooling conjugation, which are responsible for partitioning the rectangles of the entry image in a shorter set of pixels for each sub region. Among many non-linear pooling functions, the most used one is max pooling.

![image](https://github.com/FelipeMFO/supernovae-classification/assets/38300024/34db40a5-9761-4376-8ced-1b7a4cec14c7)

Figure 2.5: Example of Max pooling. The highest value in a “rectangle” is selected and disseminated to another layer, which has its dimension reduced. Images from DERTAT [25].

Inside the internal layers it is impossible to precisely affirm which is the “physical sense” of each one of them. Some may have only the mathematical meaning meanwhile others can be responsible for identifying shapes. For example, when identifying faces, some layers can be responsible for detecting eyes, feet, curves, etc.; when identifying animals, some may detect tails, snouts and paws.

It is important to state that there is a small randomness factor inside the training processes involving the internal parameters of each neuron optimization. In order to be able to compare the results, making sure that they won’t have any deviation caused by the randomness, random seeds were used, Python functions responsibles for conserving the pseudorandomness [26].


## 2.3. Other concepts
  In the ML’s vocabulary the expressions dataframe and feature are used, respectively, to refer to the amount of samples for training and testing the model; and its characteristics that will be analyzed for the classification.

### 2.3.1. Principal components analysis
The principal components analysis (PCA) uses orthogonalization of vectors so it can reduce dimensions. 

When it comes to ML, this dimension reduction is applied to shorten the group of features of a dataframe in which the samples have a great number of characteristics so that the machine learning method will be less expensive and equally efficient. 

By linear algebra techniques the method creates new virtual dimensions formed by linear combinations that already exist so that these new dimensions can better distinguish the amount of data in the dataframe. 

At last, it is also possible to interpret PCA through a perspective of filters, where a certain signal is decomposed and, while selecting certain frequencies, its potency is conserved. From this perspective, the frequencies are features and the potency is the capacity that new features have for representing the data frame. 

### 2.3.2. Wavelet Transform
  Wavelet Transform [28] or Wavelet are functions which are capable of describing or representing other functions in different scales of frequency and time. 
  
  They are normally used in the domain of signals processing, being useful to eliminate noises and to compress data, for example.
  
  In this project, the Wavelet Transform are used to process the interpolated graphics from GP, which results in values that could possibly have interpolation’s errors eliminated.
### 2.3.3. K-Fold Cross Validation
Cross validation is a family of statistical methods used to determine how a model can be generalized regardless of the training and testing set. In short, it is mostly used to analyze if a model is strong enough as well as to analyze the amount of false positives and negatives. K-Fold Cross Validation is one of these methods. 

Its procedure consists in dividing the data in K parts, usually k-1 are used for training the model while 1 is used for testing. However, in this project, due to physical limitations, only a part of it will be used for training, while the other eighteen ones will be used for testing and validating the method. 

Alongside with the cross validation, 3 techniques will be used for evaluating the results of each fold of this project:
- Confusion Matrix.
- ROC Curves and AUC (Chap. 5.7 MURPHY).
- Mean and Average Precision (Chap. 9.7.4 MURPHY).

#### Confusion Matrix
 Confusion Matrix is for representing the hits and misses of the model. The figure 2.6 illustrates the concept in a general way. The True Value is that one confirmed by analysis and the Predicted Value is that one predicted by the test. 

![image](https://github.com/FelipeMFO/supernovae-classification/assets/38300024/a5f8e53b-80e5-41ef-bf03-35f61ade24f9)

Figure 2.6: General illustration of a confusion matrix. Image from VAZ [1]
 Through the concept of True Positives (TP), False Negatives (FN) and True Negatives (TN), other concepts are defined below:

![image](https://github.com/FelipeMFO/supernovae-classification/assets/38300024/1c479fab-f9ec-43ca-8a09-9056142dc90e)

#### ROC Curve sand AUC
To be able to understand these concepts it is important to establish what is a decision threshold inside of a classification model. It establishes a “partition” between what will be classified as a type and what will be classified as another one.

When this threshold is altered, the TN, TP, FN and FP values are also altered, and, consequently, the Sensitivity and Specificity values. Illustration in figure 2.7.

![image](https://github.com/FelipeMFO/supernovae-classification/assets/38300024/c756ab89-5894-42f9-9a0f-8447fe99dba3)

Figure 2.7: Illustration of the threshold “displacement” and the modification of the graphic Sensitivity and Specificity. Image from VAZ [1].

It is remarkable that, from a certain threshold’s values, there will not be any more FN for a small value and for a high value there will not be any FP, which makes the expressions of specificity and sensibility take the value of 1. 
  Lastly, the ROC Curve (Receiver Operating Characteristic curve) is the blue curve illustrated in figure 2.7(b) and the area under this curve (Area Under Curve) is called AUC.
  The interpretation of these two indicators for the method’s efficiency consists of ROC being the most curved as possible so the AUC’s value reaches close to 1 as far as possible. That means the maximization of TN and TP (rights) and the reduction of FN and FP (errors).

#### Mean Average Precision 
  Also known as Average Precision, this is defined as the area under the curve of the Precision x Sensibility graphic over the classification of n samples (incluiding both hits and misses).

![image](https://github.com/FelipeMFO/supernovae-classification/assets/38300024/31743c07-1dd2-4e45-bece-2fa2e150d5b8)

Figure 2.8: Illustration of the Precision x Sensibility graphic to n = 7 classified samples. Image from HUI [2].

Mathematically, Mean Average Precision is defined by

- <img src="https://latex.codecogs.com/gif.latex?O_t=\text { Onset event at time bin } t " /> 
- <img src="[https://latex.codecogs.com/gif.latex?O_t=\text { Onset event at time bin } t ](https://latex.codecogs.com/svg.image?AP=%5Cint_%7B0%7D%5E%7B1%7Dp(r)dr)"/> 

![equation](https://latex.codecogs.com/svg.image?AP=%5Cint_%7B0%7D%5E%7B1%7Dp(r)dr)
(2.5)

Where r is Sensitivity and p(r) is Accuracy.

# Chapter 3. Current Model
  In this chapter will be detailed the current model used for the cosmological studies from IF-UFRJ.

## 3.1. Raw data processing
  The raw data arrive at the pre-processing in the way described in figure 3.1.
  From the pre-processing files all this information will be obtained in the form of a dictionary. Nevertheless, only the following ones will be used in this project.
- SN TYPE: informs the type of supernova.
- TERSE LIGHT CURVE OUTPUT: informs the observed aspects. This will be the dataframe.
- MJD: the moment when the information was obtained. The measure is described in days of the Modified Julian Date [29].
-  FLT: the filter in which the information was obtained. Afterall, the objects are observed in 4 bands of the light wavelength. In the figure 3.2 we can fully observe an example of an observation and how it is decomposed in 4 bands.
- FLUXCAL: the value of light flow obtained in that observation.
- FLUXCALERR: the value of the light flow’s error.

  In relation to MJD, it is relevant to explain that it is a method used in Astronomy for sequentially counting the days, starting in an arbitrary day in the past.
  
  ![image](https://github.com/FelipeMFO/supernovae-classification/assets/38300024/4a3668b9-f6d6-4b9c-b167-a553dff03cfc)

Figure 3.1: File .txt to be read by the pre-processing files.
  In this project these dates will be normalized having zero as the smallest MJD value so that the abscissa axis will always start with zero. 
  
  Regarding the FLT, each astronomical object is seen through 4 different filters of light for each one of them. 
  
  From the machine learning percpective, each object will have 4 dataframes - one for each filter - and, afterwards, these properties will be converted into features of each object. 
  
  The figure 3.3 illustrates 4 light curves (g, r, i, e, z) for the same object. The Y curve is not used in this project’s data. 

## 3.2. Current pipeline
  Laying out the data obtained from the pre-processing which was explained in the previous section, the pipeline, initially, will separate the data from the Terse Light Curve Output by the filters and will create 4 numpy arrays, each one containing n x 3 dimensions (MJD, FLUXCAL and FLUXCALERR), with n being the number of samples of each object (figure 3.4).
Figure 3.2: Full observation of an astronomic object. It does not refer to any specif example from the data frame.
  Then this 4 numpy arrays will be grouped into the form of a dictionary and they will be the entry of the Gaussian Process, whose interpolate function is a curve that passes through each filter's points 3.3.
  The output of the Gaussian Process will be the interpolated graphic in the form of numpy array 100 x 3 containing 100 abscissa points, 100 ordenate points and 100 points of the ordenate's error.
  This output will be the entry of the Wavelet Process, which will return 400 values for each entry.
  Altogether, this procedure will be repeated four times for each object, one for each filter. Thus, as the output of the Wavelet Process, there will be 1600 coeficients for each astronomic object. 
  Henceforth, it will be created a reduction of variables through the principal component analysis resulting in each object with 20 features.
   After this step, the dataframe will be formed by 21316 samples, with 20 features each. 
  In paralell, we use the SN TYPE from the dictionary to store the labels of each astronomic object in a list form. 
  Finally, there will be established parameters that caracterize the classification model based on Random Forest. In this point we use the component sklearn.pipeline from the library scikit-learn.
 (a)
(b) 
( c)
(d)
Figure 3.3: Example of 4 light curves from a same object, one for each filter. Graphic Flow x Time (days). 
Figure 3.4: Graphic ilustration of the pipeline. 
Having the dataframe, the list containing each object's classification (the labels) and the classification model in hands, we make the division through K-Fold Cross Validation Method, dividing the objects into 19 partitions and using one of them for training and the other ones for testing. The number 19 was chosen because it meets the specifications for training with approximately 1100 objects. 21316 (objects) ÷19 (folds)≈ 1122 (amount of objects destined for training).
  At last, each model of the 19 existing folds is evaluated through 2 ways of score obtaining, the AUC method (area under the ROC curve) and the average precision method 2.3. The confusion matrix of each one of the 19 models can also be analyzed to ilustrate its rights, false positives and false negatives 3.5.
(a) Example of Confusion Matrix, absolute values. 
(b) Confusion Matrix, normalized values. 
Figure 3.5: Confusion Matrix of the final model from the original pipeline. 

CHAPTER 4
Outliers Treating 
 The first way we took to search improvements in this project was through Outliers Treating. Every type of raw data is subject to samples with absurd values or unreal uncertainties, be them caused by measurement errors or any other factors.
4.1. Strategies used
The first treatment made was the removal of negative points. Since the values to be interpolated are light flows in function of time, it makes no physical sense in having negative points. However, the reason for them to exist in the records are the sensor's haziness in which, due to thermal agitation, is possible that an electron spountaneously passes from one receptor to another, resulting in a negative counting of electrons in the original receptor, provoking the negative value. 
  The second strategy was to analyze the uncertainties of the samples of each point that concerned to the astronomic object's 4 filters. Followingly, we calculated the average and the standard deviation of these groups of uncertainties and, lastly, there was the exclusion of those points whose error was higher than the average plus a standard deviation of the uncertainties distribution.
  Another attempt was the application of thresholds based on the values of the uncertanties of each point regarding the peak's curve. In case the value of the uncertainty was higher than the curve's peak multiplied by a threshold (a value between 0 and 1), this point would be excluded.
 The code which refers to the implementation of this Outlier Treatment as well as every code of the project are available on GitHub A.
 4.2. Results and comparison
There will be presented 3 different types of scores for each one of the 19 models generated by K-Fold Cross Validation: AUC Curve (AUC), Average Precision (AP) and the template of the scikit  library. The following strategies were applied:
Removal of negative values and of those with a uncertainty that is higher than the average, plus standard deviation. Presented in chart 4.1 as StdDev.
Removal of negative values and of those with an uncertainty that is higher than 70% of the peak's value. Presented in chart 4.1 as Threshold.

Chart 4.1: Results of Outliers Treatment. "Standard" refers to the score default of scikit, and the columns that do not show neither StdDev neither Threshold present the scores of the original pipeline.
The final conclusion for Outliers treatment is that it does not influence significantly in the scores rising. The possible explanation for that can be found in the robustness that Wavelet Transform offer to the method. 
  Another explanation is that the fraction of outliers is pretty low. In StdDev method only 27 outliers were removed while in Threshold that number was 85, resulting in a clean percentage of 0,13% and 0,4% of every data, respectively.

CHAPTER 5
Convolutional Neural Network
5.1. Images and parameter generator
The first step of this approach consists of generating graphics from the Gaussian Interpolation. In order to be possible to compare this method's results along with the initial pipeline, the interpolation created to generate graphics given to the Deep Learning model was the same interpolation used in the original pipeline.
  These graphics will be images in gray scales in .png extension whose dimensions are: 64 x 40 pixels. When it generated these images, they were created with 5 pixels borders in the y dimension and 9 pixels in the x dimension . In the end, they were eliminated, which resulted in a dimensional format of 46 x 30. 
  These sizes were chosen based on classic and well-known examples from the literature that have dimensions with very approximated values, such as MNIST and Fashion MNIST.
5.2. Pipeline
  After the transformation of the image files into numpy array matrices, it was built a structure of data for each object as a group formed by its 4 figures, obtaining the final format of (21316, 4, 30, 46), in which there were:
Number of objects: 21316
Number of images or matrices (referring to each filter): 4
Number of pixels per column (or number of lines): 30
Number of pixels per line (or number of columns): 46

Among the many possible types of internal layers of Deep Learning, we chose Convolucional 2D and Max Pooling 2D (section 2.2).
  The chosen Architecture was also based on classic examples of problems in the identification of images, in particular Fashion MNIST.
  Based on these examples, some layers were added and modified in a way so it could search and empirically obtain a better result. The final architecture can be found in figure 5.1. 
Figure 5.1: Archictecture of the Deep Learning method that was used.
The last value of each layer's Shape is the amount of neurons they have, while other values are the image's number of lines and columns. The value '4', which was expected due to the 4 filters, does not quite explicitly appear in the model's architecture structure. However, that is a normal behaviour since in this case it works as a image in "RGBA", where it would have one matrix for red, blue, green and transparency. 
  Followingly, it was defined a random seed in order to fixate any type of randomness intrinsic to the model. Thus, the same one was trained in 2, 3, 5 and 10 periods (this number was also based on the mentioned examples), obtaining a satisfactory result to the value of 10 periods, for its accuracy was high without making the algorithm dependent. 
5.3. Results and comparison 
  The results in shape of Confusion matrix of the best model are described below. 5.2.
(a) Normalizes values.
(b) Absulute values.
Figure 5.2: Confusion matrices of the best Deep Learning model.
 While comparing the results from figure 5.2 with those from figure 3.5, we noticed that Deep Learning revealed a worsening in its performance. 
  There are two reasons that can explain these results. The first one of them is the fact that the trained images were not subjected to what we call data augmentation, a technique which seeks to increase that amount of trained data and vary its forms of identification, focusing on rotating, shifting, reducing and enlarging the figures. As the images from the project are graphic, there would be a loss of sense if they were rotated or displaced. 
  The second reason is the physical limitation that makes us use only 1100 astronomic objects for training. Usually, Deep Learning algorithms turn up to be more advantageous than ML algorithms due to the big set of data which makes them have a better performance. 
  Lastly, experiments were also made involving a data distribution of 80% for training and 20% for testing. The analysis of these Confusion matrices (figures 5.3 and 5.4) confirm that, to this problem, the application of Deep Learning, analyzing the graphic's format, does not appear eficient, reiterating the justification of the absence of data augmentation. 
Figure 5.3: Normalized values of the DL model trained with 80% of the data.
Figure 5.4: Model of the original pipeline trained with 80% of the data.

CHAPTER 6
Gaussian Process Interpolation
The approach taken by IF-UFRJ when using Gaussian Process was through the library george, a library focused on evaluating the marginal probability of data distribution. 
  However, while trying to edit some internal parameters referred to the interpolation in this library, with Kernels or a priori functions 2.1, there was a certain difficulty, or even a impossibility, in shaping them the intended way, such as adding and multiplying Kernels, especially those non stationary ones (Chap. 3).
  One of the reasons which led us to use one or another library in the project was the quote from chapter 4 section 2 of RASMUSSEN and WILLIAMS, which argues that the Kernel of Squared Exponential Covariance Function (used in the original pipeline) provides to the interpoled function a unreal smoothing to many physical phenomena and recommends using Kernel of Matern type. Even though george library also contains this type of Kernel, other libraries had a wider possibility of editing.
  In the section below the types of the analyzed libraries will be presented.
6.1. Library Choice
The library used in MICHELLE LOCHNER's arcticle was Gapp, a library for functions reconstruction used in other cosmology works.
  However, the very same pipeline developped by IF-UFRJ already aimed to optimize M. Lochner's original article, so this project came from studies that have already been done, having as a starting point the george library .
  It is worthy to mention that even though it is possible to manually implement Gaussian Processes, the many available libraries already have the specifications and settings to the models in a more automated way. The three libraries analyzed were:
SciKit-Learn
GPflow
PyMC3

In particular, each one of this packets include a set of covariance functions which can be flexibly combined to properly describe the data's specific patterns along with the methods in order to adjust GP's parameters.
  The major part of the analysis was based on the blog Domino, where many experiments were made comparing the performance of each library.
  The details of these analysis can be seen throughout the entire project carried out by the blog mentioned above. In order to not miss the scope of this work, other details concerning the specificity of each library will not be mentioned . 
 Briefly explaining the final choice, it can be broadly stated that the scikit-learn has a simpler and less focused approach on the sophisticated probabilistic models. Meanwhile, both GPflow and PyMC3 have their on computational backend supporting these models. Finally, the reason for choosing PyMC3 was because of the wider community suport and the large domain of the library study, as for being a probabilistic programming packet the PyMC3 covers more tools which can be useful in the developpment of the probability distributions used in GP.
6.2. Randomness and Random Seeds
Before entering Kernel's choices, it is very important to explain the randomness factor inside the project. Usually, the PyMC3 library would carry out optimization methods of the a posteriori function in its GP. Yet, for being a library that requires a high computational cost, it was decided to not execute the code line which would optimize the code in favor of choosing a Kernel function that would better perform the interpolation.
   This does not mean that GP parameters will not be optimized, it is just that they will not converge to a final value, what makes them dependent of the initial conditions internally established by the computer during Markov Monte Carlo Chains.
  The alternative to this choice of project was to compare certain random seeds. Through the analysis of 11 seeds those ones whose initial conditions resulted in a bigger overfitting of the interpolations could be chosen. The figures of the comparison of the interpolations of each seed are found in Appendix B. In this context, overfitting refers to that interpolation which "forces" the resultant graphic to pass through the points so it can have variation taxes that are more abrupt and less smoothing. In the current project this is something we wish for as abrupt variations of the flow of light correspond to a expected behaviour of an explosion.
  Always searching those seeds that eliminate constant interpolations and provide an overfitting, the best ones were:
Random Seed 8
Random Seed 6
Random Seed 5
Random Seed 4
Random Seed 9
Random Seed 7

Finally, two of them were chosen for the final interpolations: the seeds 4 and 9. The justification lies in the fact that they did not show any case of interpolation keeping constant values, while the other ones still had them, even if it was less than in the original pipeline. This decision was based on the analysis of the figures from Appendix B.
6.3. Kernel Functions
In order to choose the Kernel functions that would be used in the interpolations, we followed the PyMC3 library upport. In this support there are different ways of interpolation and analysis on how Kernels directly interfere in the format of functions.
  From the data of the reference above, the followingly Kernels were chosen for the interpolation:
Quadratic exponential:

(6.1)
Quadratic rational:

(6.2)
Matern 5/2:

(6.3)
Matern 3/2

(6.4)
In these functions we have x and x' as the values of the abscissa, and α and l as hyperparameters to be adjusted. Empirically, it was noticed that l has an effect of increasing the overfitting in exchange for the increase of the error in the interpolation. The parameter l has an effect of "correlation distance", as separated points for way more than this distance has little influence on about each other while α determines how the correlation shortens with the distance x - x'.
  These hyperparameters can assume values as probabilistic distribution functions or constant functions. After many tests using constants, it was noticed that the best results were obtained using distributions. Another factor that led us to make this choice were the examples offered in the PyMC3 library support.
  The interpolations performed in order to choose which Kernel would be used are illustrated in Appendix B. The main criteria for this choice were avoiding constant interpolations and searching for overfitting. As a result, two of the best Kernels were Matern 3/2 and Matern 5/2; which is a conclusion aligned with the theory exposed in Chapter 4 section 2 of RASMUSSEN and WILLIAMS.
6.4. Other observations
  Before getting in the results of the interpolations, two important observations are made. The first one is the solution to the problem of possible negative values. Although such approach has not been adopted in this work due to the attempt of making the code have the lowest computational cost as possible,  for future works in which we may wish to interpolate with negative values, using a not negative mean function would solve this problem. 
  Many types of functions that fulfill this prerequisite are described in PyMC3.
  As an example of this we have Half-Cauchy log-likelihood.
(6.5)
Figure 6.1: Example of distribution with not negative values. 
The second observation to be made is the use of Matern function inside the same library used in george's original pipeline.
  The results of the exchange of Kernel ExpQuadr for Matern 5/2 in george library can be seen in 6.1 and 6.3.
  The justification for this performance being worse can be related to the fact that hyperparameters were constant and not distributions. Thus, if we only analyzed george's result, it would be concluded that the use of Matern is inferior to ExpQuadr. However, while making comparisons inside a library that has more support, the improvement is visibly noticed when we interpole through the proposed Kernel than through the previous one 6.2.
6.5. Results of the interpolations
  The global result of the interpolations could not be concluded due to an code error procedure, more precisely an internal memory leak during the execution of PyMC3 models in looping. The explanation and the detailment of this error will be presented in the following section. 
Table 6.1: Results of Kernel Matern 5/2 by the library george. 'Pattern' refers to the scikit's score default.
  In this section we will present some examples of interpolations between the algorithms, where it can be noticed a better interpolation using Kernel Matern in PyMC3 through the seeds. Justifying the reason why is expected to obtain better results if the processing of all the data was concluded 6.2. 
  Other examples of comparisons are described in Appendix B's notebook.
(a) original pipeline
(b) PyMC3 Exponential Quadratic seed 9
( c ) PyMC3 Matern 5/2 seed 6, overfitting
(d) PyMC3 Matern 5/2 seed 9, overfitting
(e) PyMC3 Matern 5/2 seed 6, less overfitting
(f) PyMC3 Matern 5/2 seed 9, less overfitting
Figure 6.2: Interpolations of the object SN013742 using 6 differents settings. Graphic Flow x Time (days).

(a) Normalized values
(b) Absolute values
Figure 6.3: Confusion Matrices of Matern 5/2 through library george. Results worse than the ones presented in 3.5.
6.6. Identification and Error justification
An error ocurred during the looping of the execution of the interpolation, causing the computer to stop. While inspecting the memory use through the command htop from Ubuntu, it identified a memory leak of the system.
  During an arduous investigation of the problem, there were several attempts to identify and rectify it. Among them, the main ones were:
To search possible code building errors.
To manually force the memory release using Garbage Collector packets.
Inspection through Jupyter magic's method to locate the code line with the memory leak (%% time, %%Iprun, %%mprum).
Attempt of using GPU to processing.
Use of pakets involving hash tables to reduce the memory consumed by data structure in form of dictionaries.
Use of proper libraries to manage the location and memory use.

After this inspection, the problem was found out as a internal error of the library through the creation of probabilistic models inside a looping. In short, when we instantiate a PyMC3 model inside a looping, the library keeps allocating memory in different places without 'draining out' the previous ones, which causes the leak.
  A longstanding debate over this error does not meet the matter discussed in the current work and, due to the large amount of time invested in the study of identification and solution of the problem, it was decided to no longer carry on with this approach. Howsoever, the problem has already been identified and are available in the library alongside with the links of the forums where it have been debated over.

CHAPTER 7
Conclusions
7.1 Final conclusions
The present work's main purpose was to search for ways to optimize and improve the ML techniques in the supernovas classification model using spectroscopy. From the analysis of the results it was possible to confirm that the current model has a very efficient robustness bearing in mind  the few amount of data used for training.
  As a positive aspect of the method, this project was able to add knowledge by testing differents approachs and concluding that its aplications would not offer the result any improvements.
  Inside the scope of a final project it is also worthy to mention the student's knowledge evolution and aquisition, who had a significant learning meanwhile new technologies and concepts were beind discovered, as a consequence of his researches and discussion of new ideas for the algorithm.
  While applying the data processing strategy removing the outliers, it was possible to observe that the difference in the final results was so little that it could be neglected, which leads to the conclusion that the Wavelets transform step causes a significant robustness to the algorithm, so much that the "gross" outliers did not compromise the model. 
  Concerning the Deep Learning, it was concluded that the place in which the convolutional neural networks were applied, this technique was not sufficient because the models were trained with only one-nineteenth of the data. Besides, it was also concluded that the abscence of a possible data augmentation compromises the performance even if we used 80% of the data for training. 
  Finally, the main approach, which was the search for a better execution of the Gaussian Process, could not be concluded due to problems which do not match the project's nature. Even so, from the analysis of other methods and of the results of the graphic interpolations through Kernel Mattern 5/2 by george, we concluded that the current method is already well optimized, mainly sustained by the robustness offered by Wavelets transform.
7.2. Future works
In relation to the use of machine learning in the subject matter, many other methods are used to identify astronomic objects to other purposes. Keeping these other objectives in mind, the work developed here is useful to project new ideas in those other approaches.
  When it comes to the memory leak error, the solution is left open to future works which objectify to finish the idea that was built in here. Since the hardest part of identifying the problem has already been done, it could be published in repositories and programmer's communities in search of directed support.
  Upon other libraries studied to accomplish the interpolation, after the analysis and derailment of each one of them and its properties, they were found to be interesting to works in which the use of Gaussian Process are necessary. 
  After using PyMC3 library, despite the problem occurred, it showed itself to be efficient and with a good support to model the mathematics functions, being aligned with the statistical concepts, causing a flexibility in the distribution's modeling. Therefore, it is advised for the new ones who are interested in the project to use the library, due to its big support that facilitates the theoretical understanding. The same goes for those who already have a certain knowledge and want to try more specific and customized models.
